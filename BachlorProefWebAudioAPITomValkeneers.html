<html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Bachlor Proef Web Audio API</title>

        <link rel="stylesheet" type="text/css" href="style.css">
    </head>
    <body>
        <h1>Bachlorproef Web Audio API</h1>

        <h3>overview Web Audio API</h3>

        <h4>What is the Web Audio API?</h4>
        <p>
            The Web Audio API is an API that provides high-level functions to analyze and process audio in a browser. The actual processing is handled by the implementation in the browser.
            According to <a href="#sources">[W3C]</a>, in most cases this implementation of the functions, to analyze and process audio, is written in Assembly/C/C++. But this is not a must. <a href="#sources">[W3C]</a>
        </p>

        <h4>What is the high-level programming concept behind the Web Audio API</h4>
        <p>
            The idea behind the Web Audio API is that you describe the path an audio source takes, starting from an audio input, through modules that process the audio and ending eventually to the output.
            Let's explain this in a bit more depth. The developer, who uses the Web Audio API, first defines the input source of the audio. There are several different types of input sources, who are also called source nodes.
            For example, there are sources from a file like an AudioBufferSourceNode or computer generated Source Nodes like OscillatorNode. More about the exact functionality of these source nodes can you find under *Different types of source nodes*.
            The output of these source nodes can then be processed by other nodes, this could be an effect node such as a GainNode. A GainNode does as much as adjusting the gain of the input audio. These nodes have an input and an output.
            The input of nodes can come from any type of node. The output of all nodes can also go to any type of node. This loose coupling of input and output makes a lot of different combinations possible.
            After the sound has been processed it is ready to be send to the output. Notice that the sound can be send to an output. It is perfectly possible that sound is only analyzed or processed without the need to output the audio to an audio device.
            As we will see later. Figure 1 shows the flow of the audio.<a href="#sources">[MDN]</a>
        </p>

        <figure>
            <img src="webaudioAPI_en.svg" alt="flow of Web Audio API"/>
            <figcaption> Figure 1: the basic flow of input audio to output audio. <a href="#sources">[MDN]</a></figcaption>
        </figure>

        <h3>The BaseAudioContext interface</h3>
        <p>
            The BaseAudioContext is the start point and end point of the sound processing. All rendering of audio is done inside an audio context. The context represents the total process of rendering the sound.
            It is the object that represents the connection between al the nodes and the audio output. The context also creates his nodes and connects them together. There are two implementations of this interface.
            Both of them are discussed below. <a href="#sources">[W3C]</a><a href="#sources">[MDN]</a>
        </p>
        <h5>TODO: interesting methods, events and variables</h5>
        <h4>The AudioContext interface</h4>
        <p>
            This interface inherits the BaseAudioContext interface and is used for real-time output rendering of audio. It will generate an audio signal that will be send to the user.
            Real-time in this context means that if an audio file or stream takes 1 minute to play, it will take 1 minute to process. The context will make sure that the audio is processed at the same speed as the audio would normally play.
            <a href="#sources">[W3C]</a><a href="#sources">[MDN]</a>
        </p>
        <h5>TODO: interesting methods, events and variables</h5>
        <h4>The OfflineAudioContext interface</h4>
        <p>
            This interface inherits the BaseAudioContext interface and is used for as fast as possible output rendering of audio. It will generate an audio signal as fast as possible and when the rendering of it is complete, return the audio signal in a promise.
            This audio context shows that you do not necessarily have to output the audio to an audio device. See *What is the high-level programming concept behind the Web Audio API*.
            <a href="#sources">[W3C]</a><a href="#sources">[MDN]</a>
        </p>
        <h5>TODO: interesting methods, events and variables</h5>


        <div id="sources">
            <h4>Sources:</h4>
            <p><a href="#sources">[W3C]</a> World Wide Web Consortium <a href="https://www.w3.org/TR/2015/WD-webaudio-20151208/">https://www.w3.org/TR/2015/WD-webaudio-20151208/</a></p>
            <p><a href="#sources">[MDN]</a> Mozilla Developer Network <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API">https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API</a></p>
            <p><a href="#sources">[BBC]</a> British public service broadcaster VenueExplorer, Object-Based Interactive Audio for Live Events <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.682.7501&rep=rep1&type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.682.7501&rep=rep1&type=pdf</a></p>
        </div>

        <hr/>


        <h4>To discused later</h4>
        <ul>
            <li>State of the Web Audio API - is it stil in development/beta/draft/... what functions are avalable</li>
            <li>Different types of SourceNodes</li>
            <li>Different types of ProcessNodes</li>
        </ul>

        <h4>To Ask</h4>
        <ul>
            <li>dopler effect mogelijk met API -> geluids muur</li>
            <li>engels vs nederlands</li>
            <li>html ok?</li>
            <li>bitbucket/git ok?</li>
        </ul>

    </body>
</html>
